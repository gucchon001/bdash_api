import requests
import json
import time
import base64
import pandas as pd
import os
from datetime import datetime

# ==========================================================
# è¨­å®šå€¤
# ==========================================================
# ã‚³ãƒ­ãƒ³ä»˜ãã®APIã‚­ãƒ¼ã‚’ãã®ã¾ã¾è¨­å®š
API_KEY_WITH_COLON = "100376-9907-pn6b24wh6r:dxmiwqjteehpdc1r8jfugkigiks64gi13ijietcas77f26ntvc1rcrq21g51tdgpsnrjeuswpf8i3c1wptqu5ib5hn4ar4th4y2xcme6s"
DATAFILE_ID = "503"

# ==========================================================
# APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æº–å‚™
# ==========================================================
base_url = "https://api.smart-bdash.com/api/v1"
api_endpoint = f"{base_url}/datafiles/{DATAFILE_ID}/records"

# --- Basicèªè¨¼ã®ãŸã‚ã®æº–å‚™ ---
# APIã‚­ãƒ¼ã‚’ã‚³ãƒ­ãƒ³ã§åˆ†å‰²ã—ã¦ã€IDã¨ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã«åˆ†ã‘ã‚‹
try:
    api_key_id, api_key_secret = API_KEY_WITH_COLON.split(':', 1)
    print(f"âœ… APIã‚­ãƒ¼ã‚’åˆ†å‰²ã—ã¾ã—ãŸ: ID={api_key_id[:10]}...")
    print(f"âœ… APIã‚­ãƒ¼ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆ: {api_key_secret[:10]}...")
except ValueError:
    print("âŒ ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ã«ã‚³ãƒ­ãƒ³ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
    exit(1)

# ==========================================================
# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
# ==========================================================
print("ğŸš€ bâ†’dash APIç–é€šãƒ†ã‚¹ãƒˆé–‹å§‹")
print("=" * 60)

# --- å…±é€šä»•æ§˜æº–æ‹ ã®ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®š ---
def get_common_headers():
    """å…±é€šä»•æ§˜ã«æº–æ‹ ã—ãŸãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿”ã™"""
    return {
        'Authorization': f'Bearer {API_KEY_WITH_COLON}',
        'Content-Type': 'application/json; charset=UTF-8',  # å…±é€šä»•æ§˜æº–æ‹ 
        'User-Agent': 'bdash-api-test/1.0'
    }

# --- CSVä¿å­˜æ©Ÿèƒ½ ---
def save_to_csv(data, filename):
    """å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
    if 'result' not in data:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ãŒä¸æ­£ã§ã™")
        return False
    
    result = data['result']
    header_info = result.get('header_info', [])
    records = result.get('records', [])
    
    if not header_info or not records:
        print("âŒ ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã¾ãŸã¯ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    
    # ã‚«ãƒ©ãƒ åã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆï¼ˆå†…éƒ¨ID â†’ æ—¥æœ¬èªåï¼‰
    column_mapping = {}
    for col in header_info:
        # å†…éƒ¨çš„ãªã‚«ãƒ©ãƒ IDã‚’å°æ–‡å­—ã«å¤‰æ›
        internal_id = col.get('column_id', '').lower()
        # å®Ÿéš›ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹å½¢å¼ã«åˆã‚ã›ã‚‹
        for record in records:
            for key in record.keys():
                if key.lower() == internal_id:
                    column_mapping[key] = col.get('column_name', key)
                    break
    
    # DataFrameã‚’ä½œæˆ
    df = pd.DataFrame(records)
    
    # ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªåã«å¤‰æ›
    original_columns = df.columns.tolist()
    new_columns = []
    for col in original_columns:
        japanese_name = column_mapping.get(col, col)
        new_columns.append(japanese_name)
    
    df.columns = new_columns
    
    # é…ä¿¡å¹´æœˆã§ä¸¦ã¹æ›¿ãˆï¼ˆæ˜‡é †ï¼‰
    date_column = None
    for col in df.columns:
        if 'é…ä¿¡å¹´æœˆ' in col or 'å¹´æœˆ' in col:
            date_column = col
            break
    
    if date_column:
        print(f"ğŸ“… é…ä¿¡å¹´æœˆã‚«ãƒ©ãƒ  '{date_column}' ã§æ˜‡é †ã«ä¸¦ã¹æ›¿ãˆã¾ã™")
        
        # é…ä¿¡å¹´æœˆã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›ã—ã¦ã‚½ãƒ¼ãƒˆ
        try:
            # YYYY/MMå½¢å¼ã‚’YYYY-MM-01å½¢å¼ã«å¤‰æ›ã—ã¦ã‚½ãƒ¼ãƒˆ
            df['_sort_date'] = pd.to_datetime(df[date_column] + '/01', format='%Y/%m/%d', errors='coerce')
            df_sorted = df.sort_values('_sort_date', ascending=True)
            df_sorted = df_sorted.drop('_sort_date', axis=1)  # ã‚½ãƒ¼ãƒˆç”¨ã‚«ãƒ©ãƒ ã‚’å‰Šé™¤
            df = df_sorted
            print(f"âœ… é…ä¿¡å¹´æœˆã§æ˜‡é †ã«ä¸¦ã¹æ›¿ãˆã¾ã—ãŸ")
        except Exception as e:
            print(f"âš ï¸ é…ä¿¡å¹´æœˆã®ä¸¦ã¹æ›¿ãˆã«å¤±æ•—: {e}")
            print("ğŸ“‹ å…ƒã®é †åºã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã™")
    else:
        print("âš ï¸ é…ä¿¡å¹´æœˆã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # dataãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
    os.makedirs('data', exist_ok=True)
    filepath = f'data/{filename}'
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆUTF-8 BOMä»˜ãã§ä¿å­˜ã—ã¦Excelã§ã‚‚æ­£ã—ãè¡¨ç¤ºï¼‰
    df.to_csv(filepath, index=False, encoding='utf-8-sig')
    
    print(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
    print(f"ğŸ“Š ä¿å­˜ãƒ‡ãƒ¼ã‚¿: {len(records)}è¡Œ Ã— {len(df.columns)}åˆ—")
    
    # é…ä¿¡å¹´æœˆã®ç¯„å›²ã‚’è¡¨ç¤º
    if date_column and date_column in df.columns:
        date_values = df[date_column].dropna().unique()
        if len(date_values) > 0:
            print(f"ğŸ“… é…ä¿¡å¹´æœˆã®ç¯„å›²: {min(date_values)} ï½ {max(date_values)}")
            print(f"ğŸ“Š é…ä¿¡å¹´æœˆã®ç¨®é¡: {len(date_values)}ç¨®é¡")
    
    # ã‚«ãƒ©ãƒ åãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¡¨ç¤º
    print("ğŸ“‹ ã‚«ãƒ©ãƒ åãƒãƒƒãƒ”ãƒ³ã‚°:")
    for original, japanese in zip(original_columns, new_columns):
        if original != japanese:
            print(f"  {original} â†’ {japanese}")
    
    return True

# --- ãƒ†ã‚¹ãƒˆ1: GETï¼ˆæˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰ ---
print("\nğŸ“Š ãƒ†ã‚¹ãƒˆ1: GETãƒ¡ã‚½ãƒƒãƒ‰ã§ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰")
print("-" * 50)

headers = get_common_headers()
# GETãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã¯Content-Typeã¯ä¸è¦ãªã®ã§å‰Šé™¤
del headers['Content-Type']

# ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
params = {'limit': 5000}  # 100ä»¶å–å¾—
response = requests.get(api_endpoint, headers=headers, params=params)

print(f"ğŸ“¡ ãƒªã‚¯ã‚¨ã‚¹ãƒˆ URL: {response.url}")
print(f"ğŸ“‹ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
print(f"ğŸ“„ ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ¦‚è¦: {len(response.text)} æ–‡å­—")

if response.status_code == 206:
    print("âœ… GETæˆåŠŸï¼ˆ206 Partial Contentï¼‰")
    try:
        data = response.json()
        if 'result' in data:
            print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ§‹é€ : {list(data['result'].keys())}")
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã®è¡¨ç¤º
            if 'header_info' in data['result']:
                header_info = data['result']['header_info']
                print(f"ğŸ“‹ ã‚«ãƒ©ãƒ æƒ…å ±: {len(header_info)} åˆ—")
                print("ğŸ“ ã‚«ãƒ©ãƒ ä¸€è¦§:")
                for i, col in enumerate(header_info):
                    column_name = col.get('column_name', 'Unknown')
                    column_id = col.get('column_id', 'Unknown')
                    data_type = col.get('data_type', 'Unknown')
                    print(f"  [{i+1:2d}] {column_name} ({column_id}) - {data_type}")
            
            # ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
            if 'records' in data['result']:
                records = data['result']['records']
                print(f"ğŸ“ˆ ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(records)}")
                
                # æœ€åˆã®3ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¤º
                print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3è¡Œï¼‰:")
                print("=" * 80)
                
                for i, record in enumerate(records[:3]):
                    print(f"\nğŸ” ãƒ¬ã‚³ãƒ¼ãƒ‰ {i+1}:")
                    print("-" * 40)
                    
                    # ãƒ¬ã‚³ãƒ¼ãƒ‰ã®å†…å®¹ã‚’è¦‹ã‚„ã™ãè¡¨ç¤º
                    for key, value in record.items():
                        if value is not None and value != "":
                            # å€¤ãŒé•·ã„å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
                            if isinstance(value, str) and len(value) > 50:
                                display_value = value[:50] + "..."
                            else:
                                display_value = value
                            print(f"  {key}: {display_value}")
                    
                    # ç©ºã§ãªã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ•°ã‚’è¡¨ç¤º
                    non_empty_fields = sum(1 for v in record.values() if v is not None and v != "")
                    print(f"  ğŸ’¡ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›æ¸ˆã¿ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: {non_empty_fields}/{len(record)}")
                
                print("\n" + "=" * 80)
                
                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"bdash_datafile_{DATAFILE_ID}_{timestamp}.csv"
                
                if save_to_csv(data, filename):
                    print(f"ğŸ‰ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« {DATAFILE_ID} ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸï¼")
                else:
                    print("âŒ CSVä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                # ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±
                if records:
                    total_fields = len(records[0])
                    filled_fields_per_record = [
                        sum(1 for v in record.values() if v is not None and v != "")
                        for record in records
                    ]
                    avg_filled = sum(filled_fields_per_record) / len(filled_fields_per_record)
                    
                    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
                    print(f"  - ç·ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°: {total_fields}")
                    print(f"  - å¹³å‡ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ç‡: {avg_filled:.1f}/{total_fields} ({avg_filled/total_fields*100:.1f}%)")
                    print(f"  - æœ€å¤§ãƒ‡ãƒ¼ã‚¿å…¥åŠ›æ•°: {max(filled_fields_per_record)}")
                    print(f"  - æœ€å°ãƒ‡ãƒ¼ã‚¿å…¥åŠ›æ•°: {min(filled_fields_per_record)}")
        else:
            print("ğŸ“„ ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰:")
            print(response.text[:500])
    except json.JSONDecodeError:
        print("âš ï¸ JSONè§£æã‚¨ãƒ©ãƒ¼")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
else:
    print(f"âŒ GETå¤±æ•—: {response.status_code}")

# --- ãƒ†ã‚¹ãƒˆ2ä»¥é™ã¯çœç•¥ï¼ˆãƒ‡ãƒ¼ã‚¿å–å¾—ã®ã¿ã«é›†ä¸­ï¼‰ ---
print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨CSVä¿å­˜ãƒ†ã‚¹ãƒˆå®Œäº†")
print("-" * 50)

# ==========================================================
# ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœç·æ‹¬
# ==========================================================
print("\n" + "=" * 60)
print("ğŸ¯ ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœç·æ‹¬")
print("=" * 60)

print("\nâœ… æˆåŠŸã—ãŸé …ç›®:")
print("- GETãƒ¡ã‚½ãƒƒãƒ‰ã§ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆBearerèªè¨¼ï¼‰")
print("- å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å†…å®¹ã®ç¢ºèª")
print("- ã‚«ãƒ©ãƒ æƒ…å ±ã®å–å¾—")
print("- CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ã®ä¿å­˜")

print("\nğŸ”§ APIã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±:")
print(f"- ãƒ™ãƒ¼ã‚¹URL: {base_url}")
print(f"- èªè¨¼æ–¹å¼: Bearer {API_KEY_WITH_COLON[:20]}...")
print(f"- GETæˆåŠŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: /datafiles/{DATAFILE_ID}/records")

print("\nğŸ“ ä¿å­˜å…ˆ:")
print(f"- ãƒ•ã‚©ãƒ«ãƒ€: data/")
print(f"- ãƒ•ã‚¡ã‚¤ãƒ«å: bdash_datafile_{DATAFILE_ID}_[ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—].csv")

print("\nğŸ‰ ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»CSVä¿å­˜ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")